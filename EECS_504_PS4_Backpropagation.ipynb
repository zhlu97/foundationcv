{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EECS 504 PS4: Backpropagation",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhlu97/foundationcv/blob/master/EECS_504_PS4_Backpropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ix5dQS2rUMlu",
        "colab_type": "text"
      },
      "source": [
        "#EECS 504 PS4: Backpropagation\n",
        "\n",
        "Please provide the following information \n",
        "(e.g. Andrew Owens, ahowens):\n",
        "\n",
        "[Zhaohan] [Lu], [zhaohan]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_Cst4k4tuBc",
        "colab_type": "text"
      },
      "source": [
        "# Starting\n",
        "\n",
        "Run the following code to import the modules you'll need. After your finish the assignment, remember to run all cells and save the note book to your local machine as a .ipynb file for Canvas submission."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHumIO-xt57H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from torchvision.datasets import CIFAR10\n",
        "download = not os.path.isdir('cifar-10-batches-py')\n",
        "dset_train = CIFAR10(root='.', download=download)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apEPzDNtK0MC",
        "colab_type": "text"
      },
      "source": [
        "# Problem 4.2 Multi-layer perceptron\n",
        "In this problem you will develop a two Layer neural network with fully-connected layers to perform classification, and test it out on the CIFAR-10 dataset.\n",
        "\n",
        "We train the network with a softmax loss function on the weight matrices. The network uses a ReLU nonlinearity after the first fully connected layer. In other words, the network has the following architecture:\n",
        "\n",
        "input - fully connected layer - ReLU - fully connected layer - softmax\n",
        "\n",
        "The outputs of the second fully-connected layer are the scores for each class.\n",
        "\n",
        "You cannot use any deep learning libraries such as PyTorch in this part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXfumCQ21JoK",
        "colab_type": "text"
      },
      "source": [
        "# 4.2 (a) Layers\n",
        "In this problem, implement fully connected layer, relu and softmax. Filling in all TODOs in skeleton codes will be sufficient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-ljfgMv9PHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fc_forward(x, w, b):\n",
        "    \"\"\"\n",
        "    Computes the forward pass for a fully-connected layer.\n",
        "    \n",
        "    The input x has shape (N, Din) and contains a minibatch of N\n",
        "    examples, where each example x[i] has shape (Din,).\n",
        "    \n",
        "    Inputs:\n",
        "    - x: A numpy array containing input data, of shape (N, Din)\n",
        "    - w: A numpy array of weights, of shape (Din, Dout)\n",
        "    - b: A numpy array of biases, of shape (Dout,)\n",
        "    \n",
        "    Returns a tuple of:\n",
        "    - out: output, of shape (N, Dout)\n",
        "    - cache: (x, w, b)\n",
        "    \"\"\"\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the forward pass. Store the result in out.              #\n",
        "    ###########################################################################\n",
        "    out = x.dot(w) + b\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    cache = (x, w, b)\n",
        "    return out, cache\n",
        "\n",
        "\n",
        "def fc_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Computes the backward pass for a fully_connected layer.\n",
        "    \n",
        "    Inputs:\n",
        "    - dout: Upstream derivative, of shape (N, Dout)\n",
        "    - cache: returned by your forward function. Tuple of:\n",
        "      - x: Input data, of shape (N, Din)\n",
        "      - w: Weights, of shape (Din, Dout)\n",
        "      - b: Biases, of shape (Dout,)\n",
        "      \n",
        "    Returns a tuple of:\n",
        "    - dx: Gradient with respect to x, of shape (N, Din)\n",
        "    - dw: Gradient with respect to w, of shape (Din, Dout)\n",
        "    - db: Gradient with respect to b, of shape (Dout,)\n",
        "    \"\"\"\n",
        "    x, w, b = cache\n",
        "    dx, dw, db = None, None, None\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the affine backward pass.                               #\n",
        "    ###########################################################################\n",
        "    dx = np.matmul(dout, np.transpose(w))\n",
        "    dw = np.matmul(np.transpose(x), dout)\n",
        "    db = np.sum(dout,axis = 0)\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    return dx, dw, db\n",
        "\n",
        "def relu_forward(x):\n",
        "    \"\"\"\n",
        "    Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
        "\n",
        "    Input:\n",
        "    - x: Inputs, of any shape\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - out: Output, of the same shape as x\n",
        "    - cache: x\n",
        "    \"\"\"\n",
        "    out = x\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the ReLU forward pass.                                  #\n",
        "    ###########################################################################\n",
        "    out = np.maximum(x, 0)\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    cache = x\n",
        "    return out, cache\n",
        "\n",
        "\n",
        "def relu_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Computes the backward pass for a layer of rectified linear units (ReLUs).\n",
        "\n",
        "    Input:\n",
        "    - dout: Upstream derivatives, of any shape\n",
        "    - cache: returned by your forward function. Input x, of same shape as dout\n",
        "\n",
        "    Returns:\n",
        "    - dx: Gradient with respect to x\n",
        "    \"\"\"\n",
        "    dx, x = dout, cache\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the ReLU backward pass.                                 #\n",
        "    ###########################################################################\n",
        "    dx = (x > 0) * dout\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    return dx\n",
        "\n",
        "\n",
        "def softmax_loss(x, y):\n",
        "    \"\"\"\n",
        "    Computes the loss and gradient for softmax classification.\n",
        "\n",
        "    Inputs:\n",
        "    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth\n",
        "      class for the ith input.\n",
        "    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n",
        "      0 <= y[i] < C\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - loss: Scalar giving the loss\n",
        "    - dx: Gradient of the loss with respect to x\n",
        "    \"\"\"\n",
        "    loss, dx = None, None\n",
        "    ###########################################################################\n",
        "    # TODO: Implement softmax loss                                            #\n",
        "    ###########################################################################\n",
        "    # dx = np.zeros((x.shape[0], x.shape[1]))\n",
        "    # largest = np.max(x, axis=1,keepdims = True)\n",
        "    # total = np.sum(np.exp(x - largest), axis = 1,keepdims = True)\n",
        "    # loss = 0\n",
        "    # for i in range(x.shape[0]):\n",
        "    #   qx = np.exp(x[i, :] - largest) / total\n",
        "    #   loss -= y[i] * np.log(qx)\n",
        "    #   dx[i,:] = qx - y[i]\n",
        "\n",
        "    probs = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    probs /= np.sum(probs, axis=1, keepdims=True)\n",
        "    N = x.shape[0]\n",
        "    loss = -np.sum(np.log(probs[np.arange(N), y])) / N\n",
        "    dx = probs.copy()\n",
        "    dx[np.arange(N), y] -= 1\n",
        "    dx /= N\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    return loss, dx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbFxtS3zK8oz",
        "colab_type": "text"
      },
      "source": [
        "# 4.2 (b) Softmax Classifier\n",
        "\n",
        "In this problem, implement softmax classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytvxbx9UpxVL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SoftmaxClassifier(object):\n",
        "    \"\"\"\n",
        "    A fully-connected neural network with\n",
        "    softmax loss that uses a modular layer design. We assume an input dimension\n",
        "    of D, a hidden dimension of H, and perform classification over C classes.\n",
        "\n",
        "    The architecture should be fc - relu - fc - softmax with one hidden layer\n",
        "\n",
        "    The learnable parameters of the model are stored in the dictionary\n",
        "    self.params that maps parameter names to numpy arrays.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim=3072, hidden_dim=300, num_classes=10,\n",
        "                 weight_scale=1e-3):\n",
        "        \"\"\"\n",
        "        Initialize a new network.\n",
        "\n",
        "        Inputs:\n",
        "        - input_dim: An integer giving the size of the input\n",
        "        - hidden_dim: An integer giving the size of the hidden layer, None\n",
        "          if there's no hidden layer.\n",
        "        - num_classes: An integer giving the number of classes to classify\n",
        "        - weight_scale: Scalar giving the standard deviation for random\n",
        "          initialization of the weights.\n",
        "        \"\"\"\n",
        "        self.params = {}\n",
        "        ############################################################################\n",
        "        # TODO: Initialize the weights and biases of the two-layer net. Weights    #\n",
        "        # should be initialized from a Gaussian centered at 0.0 with               #\n",
        "        # standard deviation equal to weight_scale, and biases should be           #\n",
        "        # initialized to zero. All weights and biases should be stored in the      #\n",
        "        # dictionary self.params, with fc weights and biases using the keys        #\n",
        "        # 'W' and 'b', i.e., W1, b1 for the weights and bias in the first linear   #\n",
        "        # layer, W2, b2 for the weights and bias in the second linear layer.       #\n",
        "        ############################################################################\n",
        "        self.hidden_dim = hidden_dim\n",
        "        w1 = np.random.normal(0.0, weight_scale, input_dim * hidden_dim)\n",
        "        w1 = np.reshape(w1, (input_dim, hidden_dim))\n",
        "        w2 = np.random.normal(0.0, weight_scale, hidden_dim * num_classes)\n",
        "        w2 = np.reshape(w2, (hidden_dim, num_classes))\n",
        "        self.params['W1'] = w1\n",
        "        self.params['b1'] = np.zeros(hidden_dim)\n",
        "        self.params['W2'] = w2\n",
        "        self.params['b2'] = np.zeros(num_classes)\n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "\n",
        "\n",
        "    def forwards_backwards(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Compute loss and gradient for a minibatch of data.\n",
        "\n",
        "        Inputs:\n",
        "        - X: Array of input data of shape (N, Din)\n",
        "        - y: Array of labels, of shape (N,). y[i] gives the label for X[i].\n",
        "\n",
        "        Returns:\n",
        "        If y is None, then run a test-time forward pass of the model and return:\n",
        "        - scores: Array of shape (N, C) giving classification scores, where\n",
        "          scores[i, c] is the classification score for X[i] and class c.\n",
        "\n",
        "        If y is not None, then run a training-time forward and backward pass. And\n",
        "        return a tuple of:\n",
        "        - loss: Scalar value giving the loss\n",
        "        - grads: Dictionary with the same keys as self.params, mapping parameter\n",
        "          names to gradients of the loss with respect to those parameters.\n",
        "        \"\"\"\n",
        "        scores = None\n",
        "        ############################################################################\n",
        "        # TODO: Implement the forward pass for the two-layer net, computing the    #\n",
        "        # class scores for X and storing them in the scores variable.              #\n",
        "        ############################################################################\n",
        "        l1_out, l1_cache = fc_forward(X, self.params['W1'], self.params['b1'])\n",
        "        relu_out, relu_cache = relu_forward(l1_out)\n",
        "        scores, l2_cache = fc_forward(relu_out, self.params['W2'], self.params['b2'])\n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "\n",
        "        # If y is None then we are in test mode so just return scores\n",
        "        if y is None:\n",
        "            return scores\n",
        "\n",
        "        loss, grads = 0, {}\n",
        "        ############################################################################\n",
        "        # TODO: Implement the backward pass for the two-layer net. Store the loss  #\n",
        "        # in the loss variable and gradients in the grads dictionary. Compute data #\n",
        "        # loss using softmax, and make sure that grads[k] holds the gradients for  #\n",
        "        # self.params[k].                                                          # \n",
        "        ############################################################################\n",
        "        loss, dout2 = softmax_loss(scores, y)\n",
        "        dx2, dw2, db2 = fc_backward(dout2, l2_cache)\n",
        "        grads['W2'] = dw2\n",
        "        grads['b2'] = db2\n",
        "        d_relu = relu_backward(dx2, relu_cache)\n",
        "        dx1, dw1, db1 = fc_backward(d_relu, l1_cache)\n",
        "        grads['W1'] = dw1\n",
        "        grads['b1'] = db1\n",
        "        \n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "        return loss, grads\n",
        "\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwp0waIL1h_e",
        "colab_type": "text"
      },
      "source": [
        "# 4.2(c) Training\n",
        "\n",
        "In this problem, you need to preprocess the images and set up model hyperparameters. Notice that adjust the training and val split is optional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZPtQzXGMoCg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "3ac20b0e-c5d4-42a8-e458-c20e6cf18240"
      },
      "source": [
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding=\"latin1\")\n",
        "    return dict\n",
        "\n",
        "def load_cifar10():\n",
        "    data = {}\n",
        "    meta = unpickle(\"cifar-10-batches-py/batches.meta\")\n",
        "    batch1 = unpickle(\"cifar-10-batches-py/data_batch_1\")\n",
        "    batch2 = unpickle(\"cifar-10-batches-py/data_batch_2\")\n",
        "    batch3 = unpickle(\"cifar-10-batches-py/data_batch_3\")\n",
        "    batch4 = unpickle(\"cifar-10-batches-py/data_batch_4\")\n",
        "    batch5 = unpickle(\"cifar-10-batches-py/data_batch_5\")\n",
        "    test_batch = unpickle(\"cifar-10-batches-py/test_batch\")\n",
        "    X_train = np.vstack((batch1['data'], batch2['data'], batch3['data'],\\\n",
        "                         batch4['data'], batch5['data']))\n",
        "    Y_train = np.array(batch1['labels'] + batch2['labels'] + batch3['labels'] + \n",
        "                       batch4['labels'] + batch5['labels'])\n",
        "    X_test = test_batch['data']\n",
        "    Y_test = test_batch['labels']\n",
        "    \n",
        "    #Preprocess images here                                     \n",
        "    X_train = (X_train-np.mean(X_train,axis=1,keepdims=True))/np.std(X_train,axis=1,keepdims=True)\n",
        "    X_test = (X_test-np.mean(X_test,axis=1,keepdims=True))/np.std(X_test,axis=1,keepdims=True)\n",
        "\n",
        "    data['X_train'] = X_train[:40000]\n",
        "    data['y_train'] = Y_train[:40000]\n",
        "    data['X_val'] = X_train[40000:]\n",
        "    data['y_val'] = Y_train[40000:]\n",
        "    data['X_test'] = X_test\n",
        "    data['y_test'] = Y_test\n",
        "    return data\n",
        "\n",
        "def test_network(model, X, y, num_samples=None, batch_size=100):\n",
        "    \"\"\"\n",
        "    Check accuracy of the model on the provided data.\n",
        "\n",
        "    Inputs:\n",
        "    - model: Image classifier\n",
        "    - X: Array of data, of shape (N, d_1, ..., d_k)\n",
        "    - y: Array of labels, of shape (N,)\n",
        "    - num_samples: If not None, subsample the data and only test the model\n",
        "      on num_samples datapoints.\n",
        "    - batch_size: Split X and y into batches of this size to avoid using\n",
        "      too much memory.\n",
        "\n",
        "    Returns:\n",
        "    - acc: Scalar giving the fraction of instances that were correctly\n",
        "      classified by the model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Subsample the data\n",
        "    N = X.shape[0]\n",
        "    if num_samples is not None and N > num_samples:\n",
        "        mask = np.random.choice(N, num_samples)\n",
        "        N = num_samples\n",
        "        X = X[mask]\n",
        "        y = y[mask]\n",
        "\n",
        "    # Compute predictions in batches\n",
        "    num_batches = N // batch_size\n",
        "    if N % batch_size != 0:\n",
        "        num_batches += 1\n",
        "    y_pred = []\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = (i + 1) * batch_size\n",
        "        scores = model.forwards_backwards(X[start:end])\n",
        "        y_pred.append(np.argmax(scores, axis=1))\n",
        "    y_pred = np.hstack(y_pred)\n",
        "    acc = np.mean(y_pred == y)\n",
        "\n",
        "    return acc\n",
        "\n",
        "\n",
        "def train_network(model, data, **kwargs):\n",
        "    \"\"\"\n",
        "     Required arguments:\n",
        "    - model: Image classifier\n",
        "    - data: A dictionary of training and validation data containing:\n",
        "      'X_train': Array, shape (N_train, d_1, ..., d_k) of training images\n",
        "      'X_val': Array, shape (N_val, d_1, ..., d_k) of validation images\n",
        "      'y_train': Array, shape (N_train,) of labels for training images\n",
        "      'y_val': Array, shape (N_val,) of labels for validation images\n",
        "\n",
        "    Optional arguments:\n",
        "    - learning_rate: A scalar for initial learning rate.\n",
        "    - lr_decay: A scalar for learning rate decay; after each epoch the\n",
        "      learning rate is multiplied by this value.\n",
        "    - batch_size: Size of minibatches used to compute loss and gradient\n",
        "      during training.\n",
        "    - num_epochs: The number of epochs to run for during training.\n",
        "    - print_every: Integer; training losses will be printed every\n",
        "      print_every iterations.\n",
        "    - verbose: Boolean; if set to false then no output will be printed\n",
        "      during training.\n",
        "    - num_train_samples: Number of training samples used to check training\n",
        "      accuracy; default is 1000; set to None to use entire training set.\n",
        "    - num_val_samples: Number of validation samples to use to check val\n",
        "      accuracy; default is None, which uses the entire validation set.\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    learning_rate =  kwargs.pop('learning_rate', 1e-3)\n",
        "    lr_decay = kwargs.pop('lr_decay', 1.0)\n",
        "    batch_size = kwargs.pop('batch_size', 100)\n",
        "    num_epochs = kwargs.pop('num_epochs', 10)\n",
        "    num_train_samples = kwargs.pop('num_train_samples', 1000)\n",
        "    num_val_samples = kwargs.pop('num_val_samples', None)\n",
        "    print_every = kwargs.pop('print_every', 10)   \n",
        "    verbose = kwargs.pop('verbose', True)\n",
        "    \n",
        "    epoch = 0\n",
        "    best_val_acc = 0\n",
        "    best_params = {}\n",
        "    loss_history = []\n",
        "    train_acc_history = []\n",
        "    val_acc_history = []\n",
        "    \n",
        "    \n",
        "    num_train = data['X_train'].shape[0]\n",
        "    iterations_per_epoch = max(num_train // batch_size, 1)\n",
        "    num_iterations = num_epochs * iterations_per_epoch\n",
        "    \n",
        "\n",
        "    \n",
        "    for t in range(num_iterations):\n",
        "        # Make a minibatch of training data\n",
        "        batch_mask = np.random.choice(num_train, batch_size)\n",
        "        X_batch = data['X_train'][batch_mask]\n",
        "        y_batch = data['y_train'][batch_mask]\n",
        "        \n",
        "        # Compute loss and gradient\n",
        "        loss, grads = model.forwards_backwards(X_batch, y_batch)\n",
        "        loss_history.append(loss)\n",
        "\n",
        "        # Perform a parameter update\n",
        "        for p, w in model.params.items():\n",
        "            model.params[p] = w - grads[p]*learning_rate\n",
        "          \n",
        "        # Print training loss\n",
        "        if verbose and t % print_every == 0:\n",
        "            print('(Iteration %d / %d) loss: %f' % (\n",
        "                   t + 1, num_iterations, loss_history[-1]))\n",
        "         \n",
        "        # At the end of every epoch, increment the epoch counter and decay\n",
        "        # the learning rate.\n",
        "        epoch_end = (t + 1) % iterations_per_epoch == 0\n",
        "        if epoch_end:\n",
        "            epoch += 1\n",
        "            learning_rate *= lr_decay\n",
        "        \n",
        "        # Check train and val accuracy on the first iteration, the last\n",
        "        # iteration, and at the end of each epoch.\n",
        "        first_it = (t == 0)\n",
        "        last_it = (t == num_iterations - 1)\n",
        "        if first_it or last_it or epoch_end:\n",
        "            train_acc = test_network(model, data['X_train'], data['y_train'],\n",
        "                num_samples= num_train_samples)\n",
        "            val_acc = test_network(model, data['X_val'], data['y_val'],\n",
        "                num_samples=num_val_samples)\n",
        "            train_acc_history.append(train_acc)\n",
        "            val_acc_history.append(val_acc)\n",
        "\n",
        "            if verbose:\n",
        "                print('(Epoch %d / %d) train acc: %f; val_acc: %f' % (\n",
        "                       epoch, num_epochs, train_acc, val_acc))\n",
        "\n",
        "            # Keep track of the best model\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                best_params = {}\n",
        "                for k, v in model.params.items():\n",
        "                    best_params[k] = v.copy()\n",
        "        \n",
        "    model.params = best_params\n",
        "        \n",
        "    return model, train_acc_history, val_acc_history\n",
        "        \n",
        "\n",
        "# load data\n",
        "data = load_cifar10() \n",
        "train_data = { k: data[k] for k in ['X_train', 'y_train', \n",
        "                                    'X_val', 'y_val']}\n",
        "#######################################################################\n",
        "# TODO: Set up model hyperparameters                                  #\n",
        "#######################################################################\n",
        "\n",
        "# initialize model\n",
        "model = SoftmaxClassifier(hidden_dim = 300, weight_scale=1e-2)\n",
        "\n",
        "# start training    \n",
        "model, train_acc_history, val_acc_history = train_network(\n",
        "    model, train_data, learning_rate = 1e-2,\n",
        "    lr_decay= 1.0, num_epochs=10, \n",
        "    batch_size=100, print_every=1000)\n",
        "#######################################################################\n",
        "#                         END OF YOUR CODE                            #\n",
        "#######################################################################\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(Iteration 1 / 4000) loss: 2.298452\n",
            "(Epoch 0 / 10) train acc: 0.101000; val_acc: 0.112100\n",
            "(Epoch 1 / 10) train acc: 0.380000; val_acc: 0.366000\n",
            "(Epoch 2 / 10) train acc: 0.411000; val_acc: 0.410700\n",
            "(Iteration 1001 / 4000) loss: 1.598177\n",
            "(Epoch 3 / 10) train acc: 0.450000; val_acc: 0.435500\n",
            "(Epoch 4 / 10) train acc: 0.466000; val_acc: 0.451500\n",
            "(Epoch 5 / 10) train acc: 0.499000; val_acc: 0.464800\n",
            "(Iteration 2001 / 4000) loss: 1.330897\n",
            "(Epoch 6 / 10) train acc: 0.520000; val_acc: 0.475300\n",
            "(Epoch 7 / 10) train acc: 0.524000; val_acc: 0.481500\n",
            "(Iteration 3001 / 4000) loss: 1.399538\n",
            "(Epoch 8 / 10) train acc: 0.548000; val_acc: 0.484400\n",
            "(Epoch 9 / 10) train acc: 0.551000; val_acc: 0.494500\n",
            "(Epoch 10 / 10) train acc: 0.571000; val_acc: 0.495200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcovGmpXvXXa",
        "colab_type": "text"
      },
      "source": [
        "# 4.2(c) Report Accuracy\n",
        "\n",
        "Run the given code and report the accuracy on test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwCq8pBhu6dz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bff7aa67-72b0-40d3-e63b-b505c667e963"
      },
      "source": [
        "# report test accuracy\n",
        "acc = test_network(model, data['X_test'], data['y_test'])\n",
        "print(\"Test accuracy: {}\".format(acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.4948\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTrmbULS7i2N",
        "colab_type": "text"
      },
      "source": [
        "# 4.2(d) Plot\n",
        "\n",
        "Using the train_acc_history and val_acc_history, plot the train & val accuracy versus epochs on one plot. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPjtnbya9S7g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "f0699179-6ffc-4bc2-e832-969ec5856953"
      },
      "source": [
        "plt.plot(np.arange(len(train_acc_history)), train_acc_history, label='Train')\n",
        "plt.plot(np.arange(len(val_acc_history)),val_acc_history, label='Val')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fe1295a81d0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noIuIRAkFuohIlFCgi4hEif8PTNWPZ1WQuQUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}